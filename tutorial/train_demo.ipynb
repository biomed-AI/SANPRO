{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Run all</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step1:加载库与函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG']=':4096:8'\n",
    "import sys\n",
    "sys.path.append('/home/chenjn/rna2adt')\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "from sklearn.metrics import adjusted_rand_score as ARI\n",
    "from sklearn.metrics import normalized_mutual_info_score as NMI\n",
    "from sklearn.metrics import fowlkes_mallows_score as FMI\n",
    "from sklearn.metrics import silhouette_score as SC\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import dataset\n",
    "import dataloaders\n",
    "import scanpy as sc\n",
    "import scbasset_ori as scbasset\n",
    "import sklearn\n",
    "from biock import make_directory, make_logger, get_run_info\n",
    "from biock.pytorch import model_summary, set_seed\n",
    "from biock import HG19_FASTA_H5, HG38_FASTA_H5\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter(\"logs/All/epoch_20\")\n",
    "\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.sparse import issparse\n",
    "\n",
    "from utils import find_res_label\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_acc(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate clustering accuracy. Require scikit-learn installed\n",
    "\n",
    "    # Arguments\n",
    "        y: true labels, numpy.array with shape `(n_samples,)`\n",
    "        y_pred: predicted labels, numpy.array with shape `(n_samples,)`\n",
    "\n",
    "    # Return\n",
    "        accuracy, in [0,1]\n",
    "    \"\"\"\n",
    "    assert y_pred.size == y_true.size\n",
    "    \n",
    "    encoder = LabelEncoder()\n",
    "    encoder = encoder.fit(np.unique(y_true))\n",
    "    y_true = encoder.transform(y_true).astype(np.int64)\n",
    "    y_pred = y_pred.astype(np.int64)\n",
    "\n",
    "    D = max(y_pred.max(), y_true.max()) + 1\n",
    "    w = np.zeros((D, D), dtype=np.int64)\n",
    "\n",
    "    for i in range(y_pred.size):\n",
    "        w[y_pred[i], y_true[i]] += 1\n",
    "\n",
    "    # ind = linear_assignment(w.max() - w)\n",
    "    ind = linear_sum_assignment(w.max() - w)\n",
    "    ind = np.array((ind[0], ind[1])).T\n",
    "\n",
    "    return sum([w[i, j] for i, j in ind]) * 1.0 / y_pred.size\n",
    "\n",
    "\n",
    "def label_scores(embeddings, labels):\n",
    "    n_neigh = min(20, len(embeddings) // 3)\n",
    "    nn_ = NearestNeighbors(n_neighbors=n_neigh)\n",
    "    nn_.fit(embeddings)\n",
    "    knns = nn_.kneighbors(embeddings, return_distance=False)\n",
    "\n",
    "    res = 0\n",
    "    for i in range(len(embeddings)):\n",
    "        num = 0\n",
    "        for j in range(len(knns[i])):\n",
    "            if labels[i] == labels[knns[i][j]]:\n",
    "                num += 1\n",
    "        res += num / len(knns[i])\n",
    "\n",
    "    return res / len(embeddings)\n",
    "\n",
    "\n",
    "def get_CSS(data1, data2, dim=1,  func=cosine):\n",
    "    r1, p1 = [], []\n",
    "    # print(data1.shape, data2.shape)\n",
    "    for g in range(data1.shape[dim]):\n",
    "        if dim == 1:\n",
    "            # print(np.sum(data1[:, g]), np.sum(data2[:, g]))\n",
    "            r = func(data1[:, g], data2[:, g])\n",
    "        elif dim == 0:\n",
    "            # print(np.sum(data1[g, :]), np.sum(data2[g, :]))\n",
    "            r = func(data1[g, :], data2[g, :])\n",
    "        # print(r)\n",
    "        r1.append(r)\n",
    "    r1 = np.array(r1)\n",
    "    return np.mean(r1)\n",
    "\n",
    "\n",
    "def get_R(data1, data2, dim=1, func=pearsonr):\n",
    "    r1, p1 = [], []\n",
    "    # print(data1.shape, data2.shape)\n",
    "    for g in range(data1.shape[dim]):\n",
    "        if dim == 1:\n",
    "            # print(np.isnan(data1[:, g]).any())\n",
    "            # print(np.isnan(data2[:, g]).any())\n",
    "            # print(np.isinf(data1[:, g]).any())\n",
    "            # print(np.isinf(data2[:, g]).any())\n",
    "            # print(np.sum(data1[:, g]), np.sum(data2[:, g]))\n",
    "            r, pv = func(data1[:, g], data2[:, g])\n",
    "        elif dim == 0:\n",
    "            # print(np.isnan(data1[g, :]).any())\n",
    "            # print(np.isnan(data2[g, :]).any())\n",
    "            # print(np.isinf(data1[g, :]).any())\n",
    "            # print(np.isinf(data2[g, :]).any())\n",
    "            # print(np.sum(data1[g, :]), np.sum(data2[g, :]))\n",
    "            r, pv = func(data1[g, :], data2[g, :])\n",
    "        # print(r)\n",
    "        r1.append(r)\n",
    "        p1.append(pv)\n",
    "    r1 = np.array(r1)\n",
    "    p1 = np.array(p1)\n",
    "\n",
    "    return r1, p1\n",
    "\n",
    "\n",
    "def test_model(model, loader, device, epoch):\n",
    "    model.eval()\n",
    "    all_label = list()\n",
    "    all_pred = list()\n",
    "\n",
    "    for it, (seq, adt) in enumerate(tqdm(loader)):\n",
    "        seq = seq.to(device)\n",
    "        output = model(seq)[0].detach()\n",
    "        output = torch.sigmoid(output).cpu().numpy().astype(np.float16)\n",
    "\n",
    "        adt = adt.numpy().astype(np.float16)\n",
    "\n",
    "        all_pred.append(output)\n",
    "        all_label.append(adt)\n",
    "\n",
    "    all_pred = np.concatenate(all_pred, axis=0)\n",
    "    all_label = np.concatenate(all_label, axis=0)\n",
    "\n",
    "    R = get_R(all_pred, all_label, dim=0)[0]\n",
    "    R1 = get_R(all_pred, all_label, dim=1)[0]\n",
    "    print(\"T2\",all_pred.shape,all_label.shape)\n",
    "\n",
    "    css = get_CSS(all_pred, all_label, dim=0)\n",
    "    css1 = get_CSS(all_pred, all_label, dim=1)\n",
    "\n",
    "\n",
    "    def mseloss(y_true, y_pred):\n",
    "        y_true = np.array(y_true)\n",
    "        y_pred = np.array(y_pred)\n",
    "        mse = np.mean((y_true - y_pred) ** 2)\n",
    "        return mse\n",
    "    All_Loss = mseloss(all_pred, all_label)\n",
    "\n",
    "    R = np.nanmean(R)\n",
    "    R1 = np.nanmean(R1)\n",
    "\n",
    "    embedding = model.get_embedding().detach().cpu().numpy().astype(np.float32)\n",
    "\n",
    "    adata1 = sc.AnnData(\n",
    "        embedding,\n",
    "        obs=adtT.obs,\n",
    "    )\n",
    "    # print(adata1)\n",
    "    sc.pp.neighbors(adata1, use_rep='X')\n",
    "    sc.tl.umap(adata1)\n",
    "\n",
    "    sc.tl.louvain(adata1)\n",
    "    # adata1.obs['louvain_res'] = find_res_label(adata1, len(np.unique(adata1.obs[label_key])))\n",
    "\n",
    "    if label_key is not None:\n",
    "        ari = ARI(adata1.obs['louvain'], adata1.obs[label_key])\n",
    "        nmi = NMI(adata1.obs['louvain'], adata1.obs[label_key])\n",
    "        ca = cluster_acc(adata1.obs[label_key].to_numpy(), adata1.obs['louvain'].values.to_numpy())\n",
    "        fmi = FMI(adata1.obs['louvain'], adata1.obs[label_key])\n",
    "        sci = SC(adata1.X, adata1.obs['louvain'].values.reshape(-1, 1))\n",
    "        lsi = label_scores(embedding, adata1.obs[label_key])\n",
    "    else:\n",
    "        ari = 0.\n",
    "        nmi = 0.\n",
    "        ca = 0.\n",
    "        fmi = 0.\n",
    "        sci = SC(adata1.X, adata1.obs['louvain'].values.reshape(-1, 1))\n",
    "        lsi = 0.\n",
    "\n",
    "    # ari_res = ARI(adata1.obs['louvain_res'], adata1.obs[label_key])\n",
    "    # nmi_res = NMI(adata1.obs['louvain_res'], adata1.obs[label_key])\n",
    "    # ca_res = cluster_acc(adata1.obs[label_key].to_numpy(), adata1.obs['louvain_res'].values.to_numpy())\n",
    "    # fmi_res = FMI(adata1.obs['louvain_res'], adata1.obs[label_key])\n",
    "    # sci_res = SC(adata1.X, adata1.obs['louvain_res'].values.reshape(-1, 1))\n",
    "    sci_res = None\n",
    "\n",
    "    if epoch is not None:\n",
    "        print('ARI: ' + str(ari) + ', NMI: ' + str(nmi) + ', CA: ' + str(ca) + ', FMI: ', str(fmi))\n",
    "        print('SCI: ' + str(sci) + ', LSI: ' + str(lsi) + ', css: ', str(css) + ', css1: ', str(css1) + ', All_loss', All_Loss)\n",
    "        # print('ARI: ' + str(ari_res) + ', NMI: ' + str(nmi_res) + ', CA: ' + str(ca_res) + ', FMI', str(fmi_res) + ', SCI', str(sci_res))\n",
    "\n",
    "        writer.add_scalar('ARI', ari, global_step=epoch)\n",
    "        writer.add_scalar('NMI', nmi, global_step=epoch)\n",
    "        writer.add_scalar('CA', ca, global_step=epoch)\n",
    "        writer.add_scalar('FMI', fmi, global_step=epoch)\n",
    "        writer.add_scalar('SC', sci, global_step=epoch)\n",
    "        writer.add_scalar('PCC0', R, global_step=epoch)\n",
    "        writer.add_scalar('PCC1', R1, global_step=epoch)\n",
    "        writer.add_scalar('lsi', lsi, global_step=epoch)\n",
    "        writer.add_scalar('css', css, global_step=epoch)\n",
    "        writer.add_scalar('css1', css1, global_step=epoch)\n",
    "        writer.add_scalar('All_Loss', All_Loss, global_step=epoch)\n",
    "\n",
    "    return R, R1, sci, sci_res, embedding\n",
    "\n",
    "\n",
    "def split_dataset(length, tr, va):\n",
    "    seq = np.random.permutation(np.arange(length))\n",
    "    trs = seq[:int(length * tr)]\n",
    "    vas = seq[int(length * tr) : int(length * (tr + va))]\n",
    "    tes = seq[int(length * (tr + va)):]\n",
    "\n",
    "    return trs, vas, tes\n",
    "\n",
    "\n",
    "def plot_ump(adata,label_key,clust_way=\"louvain\"):\n",
    "    sc.pl.umap(adata, color=label_key)\n",
    "    sc.pl.umap(adata, color='louvain')\n",
    "\n",
    "def clusters_val(adata,label_key,clust_way=\"louvain\"):\n",
    "    print(\"=\"*100)\n",
    "    print(\"=\"*100)\n",
    "    print(\"=\"*10,clust_way,\"=\"*10)\n",
    "    print(\"ARI:\",ARI(adata.obs[clust_way], adata.obs[label_key]))\n",
    "    print(\"NMI\",NMI(adata.obs[clust_way], adata.obs[label_key]))\n",
    "    print(\"CA:\",cluster_acc(adata.obs[label_key].to_numpy(), adata.obs[clust_way].values.to_numpy()))\n",
    "    print(\"FNI:\",FMI(adata.obs[clust_way], adata.obs[label_key]))\n",
    "    print()\n",
    " \n",
    " \n",
    "    if issparse(adata.X):\n",
    "        dense_X = adata.X.toarray()\n",
    "    else:\n",
    "        dense_X = adata.X\n",
    "    print(\"Label_Score:\",label_scores(dense_X,adata.obs[label_key]))\n",
    "    print(\"SC\",SC(adata.X, adata.obs['louvain']))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2:超参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 1344\n",
    "batch_size = 4\n",
    "num_workers = 1\n",
    "z_dim = 256 \n",
    "lr = 0.01\n",
    "max_epoch = 20\n",
    "batch = None # ['P1', 'P2']\n",
    "seed = 3407\n",
    "# seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(seed, force_deterministic=True)\n",
    "outdir = make_directory('./output')\n",
    "device = torch.device(\"cuda:7\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step3:设置数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(data_name,adtT,batch_size,num_workers,lr=1e-3,max_epoch=100,label_key=None):\n",
    "    train_loader = DataLoader(\n",
    "        adtT,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "        prefetch_factor=4\n",
    "    )\n",
    "\n",
    "    model = scbasset.scBasset(n_cells=adtT.X.shape[1], hidden_size=z_dim, seq_len=seq_len, batch_ids=adtT.batche_ids).to(device)\n",
    "\n",
    "    load = False\n",
    "    if not load:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "        criterion = nn.MSELoss()\n",
    "        scaler = GradScaler()\n",
    "\n",
    "        best_sci = 0\n",
    "        best_embedding = None\n",
    "\n",
    "        max_epoch = max_epoch\n",
    "        for epoch in range(max_epoch):\n",
    "            pool = [np.nan for _ in range(10)]\n",
    "            pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{max_epoch}\")\n",
    "            epoch_loss = 0;num_batches=0\n",
    "            model.train()\n",
    "            for it, (seq, adt) in enumerate(pbar):\n",
    "                seq, adt = seq.to(device), adt.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                with autocast():\n",
    "                    output = model(seq)[0]\n",
    "                    # print(output[0], adt[0])\n",
    "                    loss = criterion(output, adt)\n",
    "                    # print(loss)\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "                num_batches += 1\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                pool[it % 10] = loss.item()\n",
    "\n",
    "                lr = optimizer.param_groups[-1][\"lr\"]\n",
    "                pbar.set_postfix_str(f\"loss/lr={np.nanmean(pool):.4f}/{lr:.3e}\")\n",
    "            \n",
    "            avg_loss = epoch_loss / num_batches\n",
    "            writer.add_scalar('Loss/train', avg_loss, epoch)\n",
    "            \n",
    "            if epoch % 1 == 0: \n",
    "                pcc0, pcc1, sci, sci_res, embedding = test_model(model, train_loader, device, epoch)\n",
    "                \n",
    "                if sci > best_sci:\n",
    "                    best_sci = sci\n",
    "                    best_epoch = epoch\n",
    "                    best_embedding = embedding\n",
    "\n",
    "\n",
    "    embedding = best_embedding\n",
    "    adata1 = sc.AnnData(\n",
    "        embedding,\n",
    "        obs=adtT.obs,\n",
    "    )\n",
    "    sc.pp.neighbors(adata1, use_rep='X')\n",
    "    sc.tl.louvain(adata1)\n",
    "    # sc.tl.louvain(adata1, random_state=seed)6\n",
    "    # adata1.obs['louvain_res'] = find_res_label(adata1, len(np.unique(adata1.obs[label_key])))\n",
    "    adata1.write(f\"/home/chenjn/rna2adt/A_run_test/new_emb_sc/{data_name}.h5ad\")\n",
    "\n",
    "    clusters_val(adata1,label_key,clust_way=\"louvain\")\n",
    "    print(f\"Best epoch{best_epoch}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    sc.tl.umap(adata1)\n",
    "    plot_ump(adata1,label_key,clust_way=\"louvain\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step4:Run(reap2 训练数据 early_stop机制)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_names = [\"pbmc\"] \n",
    "label_keys = ['celltype.l2']\n",
    "\n",
    "for i in range(len(data_names)):\n",
    "    data = data_names[i]\n",
    "    label_key = label_keys[i]\n",
    "    adt_data = '/home/chenjn/rna2adt/data/' + data + '/ADT.h5ad'\n",
    "    ref_data = '/home/chenjn/rna2adt/data/pbmc/CCND.csv'\n",
    "\n",
    "    tem_data = sc.read_h5ad(adt_data)\n",
    "\n",
    "    print(\"=\"*30,f\"P{i}:{data}\",\"=\"*30)\n",
    "    print(f\"Cell:{tem_data.shape[0]}    ADT:{tem_data.shape[1]}\")\n",
    "    # print(tem_data)\n",
    "    print(\"-\"*60)\n",
    "\n",
    "    \n",
    "    adtT = dataset.SingleCellDataset(\n",
    "        data=dataset.load_adata(adt_data, hvg=False, log1p=False, nor=False), \n",
    "        seq_ref=dataset.load_csv(ref_data),\n",
    "        seq_len=seq_len, \n",
    "        batch=batch,\n",
    "    )\n",
    "\n",
    "    run(data,adtT,batch_size,num_workers,lr=lr,max_epoch=max_epoch,label_key=label_key)\n",
    "\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rna2adt",
   "language": "python",
   "name": "rna2adt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

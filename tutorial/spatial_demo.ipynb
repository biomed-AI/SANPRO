{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG']=':4096:8'\n",
    "# import sys\n",
    "# # sys.path.append('/home/chenjn/rna2adt')\n",
    "# sys.path.append('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "from sklearn.metrics import adjusted_rand_score as ARI\n",
    "from sklearn.metrics import normalized_mutual_info_score as NMI\n",
    "from sklearn.metrics import fowlkes_mallows_score as FMI\n",
    "from sklearn.metrics import silhouette_score as SC\n",
    "import dataset\n",
    "import dataloaders\n",
    "import scanpy as sc\n",
    "import scbasset_ori as scbasset\n",
    "import sklearn\n",
    "from utils import get_R\n",
    "from biock import make_directory, make_logger, get_run_info\n",
    "from biock.pytorch import model_summary, set_seed\n",
    "from biock import HG19_FASTA_H5, HG38_FASTA_H5\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "from utils import find_res_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_acc(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate clustering accuracy. Require scikit-learn installed\n",
    "\n",
    "    # Arguments\n",
    "        y: true labels, numpy.array with shape `(n_samples,)`\n",
    "        y_pred: predicted labels, numpy.array with shape `(n_samples,)`\n",
    "\n",
    "    # Return\n",
    "        accuracy, in [0,1]\n",
    "    \"\"\"\n",
    "    assert y_pred.size == y_true.size\n",
    "    \n",
    "    encoder = LabelEncoder()\n",
    "    encoder = encoder.fit(np.unique(y_true))\n",
    "    y_true = encoder.transform(y_true).astype(np.int64)\n",
    "    y_pred = y_pred.astype(np.int64)\n",
    "\n",
    "    D = max(y_pred.max(), y_true.max()) + 1\n",
    "    w = np.zeros((D, D), dtype=np.int64)\n",
    "\n",
    "    for i in range(y_pred.size):\n",
    "        w[y_pred[i], y_true[i]] += 1\n",
    "\n",
    "    # ind = linear_assignment(w.max() - w)\n",
    "    ind = linear_sum_assignment(w.max() - w)\n",
    "    ind = np.array((ind[0], ind[1])).T\n",
    "\n",
    "    return sum([w[i, j] for i, j in ind]) * 1.0 / y_pred.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_scores(embeddings, labels):\n",
    "    nn_ = NearestNeighbors(n_neighbors=20)\n",
    "    nn_.fit(embeddings)\n",
    "    knns = nn_.kneighbors(embeddings, return_distance=False)\n",
    "\n",
    "    res = 0\n",
    "    for i in range(len(embeddings)):\n",
    "        num = 0\n",
    "        for j in range(len(knns[i])):\n",
    "            if labels[i] == labels[knns[i][j]]:\n",
    "                num += 1\n",
    "        res += num / len(knns[i])\n",
    "\n",
    "    return res / len(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, loader, device, epoch):\n",
    "    model.eval()\n",
    "    all_label = list()\n",
    "    all_pred = list()\n",
    "\n",
    "    for it, (seq, adt) in enumerate(tqdm(loader)):\n",
    "        seq = seq.to(device)\n",
    "        output = model(seq)[0].detach()\n",
    "        output = torch.sigmoid(output).cpu().numpy().astype(np.float16)\n",
    "\n",
    "        adt = adt.numpy().astype(np.float16)\n",
    "\n",
    "        all_pred.append(output)\n",
    "        all_label.append(adt)\n",
    "\n",
    "    all_pred = np.concatenate(all_pred, axis=0)\n",
    "    all_label = np.concatenate(all_label, axis=0)\n",
    "\n",
    "    R = get_R(all_pred, all_label, dim=0)[0]\n",
    "    R1 = get_R(all_pred, all_label, dim=1)[0]\n",
    "\n",
    "    R = np.nanmean(R)\n",
    "    R1 = np.nanmean(R1)\n",
    "\n",
    "    embedding = model.get_embedding().detach().cpu().numpy().astype(np.float32)\n",
    "\n",
    "    adata1 = sc.AnnData(\n",
    "        embedding,\n",
    "        obs=adtT.obs,\n",
    "    )\n",
    "    sc.pp.neighbors(adata1, use_rep='X')\n",
    "    sc.tl.umap(adata1)\n",
    "\n",
    "    sc.tl.louvain(adata1)\n",
    "    adata1.obs['louvain_res'] = find_res_label(adata1, len(np.unique(adata1.obs[label_key])))\n",
    "\n",
    "    ari = ARI(adata1.obs['louvain'], adata1.obs[label_key])\n",
    "    nmi = NMI(adata1.obs['louvain'], adata1.obs[label_key])\n",
    "    ca = cluster_acc(adata1.obs[label_key].to_numpy(), adata1.obs['louvain'].values.to_numpy())\n",
    "    fmi = FMI(adata1.obs['louvain'], adata1.obs[label_key])\n",
    "    sci = SC(adata1.X, adata1.obs['louvain'].values.reshape(-1, 1))\n",
    "    lsi = label_scores(embedding, adata1.obs[label_key])\n",
    "\n",
    "    ari_res = ARI(adata1.obs['louvain_res'], adata1.obs[label_key])\n",
    "    nmi_res = NMI(adata1.obs['louvain_res'], adata1.obs[label_key])\n",
    "    ca_res = cluster_acc(adata1.obs[label_key].to_numpy(), adata1.obs['louvain_res'].values.to_numpy())\n",
    "    fmi_res = FMI(adata1.obs['louvain_res'], adata1.obs[label_key])\n",
    "    sci_res = SC(adata1.X, adata1.obs['louvain_res'].values.reshape(-1, 1))\n",
    "\n",
    "    if epoch is not None:\n",
    "        print('ARI: ' + str(ari) + ', NMI: ' + str(nmi) + ', CA: ' + str(ca) + ', FMI', str(fmi) + ', SCI', str(sci) + ', LSI', str(lsi))\n",
    "        print('ARI: ' + str(ari_res) + ', NMI: ' + str(nmi_res) + ', CA: ' + str(ca_res) + ', FMI', str(fmi_res) + ', SCI', str(sci_res))\n",
    "\n",
    "        # writer.add_scalar('ARI', ari, global_step=epoch)\n",
    "        # writer.add_scalar('NMI', nmi, global_step=epoch)\n",
    "        # writer.add_scalar('CA', ca, global_step=epoch)\n",
    "        # writer.add_scalar('FMI', fmi, global_step=epoch)\n",
    "        # writer.add_scalar('SC', sci, global_step=epoch)\n",
    "        # writer.add_scalar('PCC0', R, global_step=epoch)\n",
    "        # writer.add_scalar('PCC1', R1, global_step=epoch)\n",
    "\n",
    "    return R, R1, sci, sci_res, embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(length, tr, va):\n",
    "    seq = np.random.permutation(np.arange(length))\n",
    "    trs = seq[:int(length * tr)]\n",
    "    vas = seq[int(length * tr) : int(length * (tr + va))]\n",
    "    tes = seq[int(length * (tr + va)):]\n",
    "\n",
    "    return trs, vas, tes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Haperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 1344\n",
    "batch_size = 4\n",
    "num_workers = 2\n",
    "z_dim = 256 \n",
    "lr = 0.01\n",
    "max_epoch = 500\n",
    "batch=None\n",
    "seed = 3407"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(seed, force_deterministic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdir = make_directory('./output')\n",
    "# logger = make_logger(title=\"\", filename=os.path.join(outdir, \"train.log\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adt_data = '/home/chenjn/rna2adt/data/karen2018a_5/ADT.h5ad'\n",
    "ref_data = '/home/chenjn/rna2adt/data/pbmc/CCND.csv'\n",
    "label_key = 'immuneGroup_name'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adtT = dataset.SingleCellDataset(\n",
    "    data=dataset.load_adata(adt_data, log1p=False, nor=False), \n",
    "    seq_ref=dataset.load_csv(ref_data),\n",
    "    seq_len=seq_len, \n",
    "    batch=batch,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = sc.AnnData(\n",
    "    adtT.X.T,\n",
    "    obs=adtT.obs,\n",
    ")\n",
    "sc.pp.neighbors(adata, use_rep=\"X\")\n",
    "sc.tl.umap(adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.tl.louvain(adata, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(adata, color='louvain')\n",
    "sc.pl.umap(adata, color=label_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ARI(adata.obs['louvain'], adata.obs[label_key]))\n",
    "print(NMI(adata.obs['louvain'], adata.obs[label_key]))\n",
    "print(cluster_acc(adata.obs[label_key].to_numpy(), adata.obs['louvain'].values.to_numpy()))\n",
    "print(FMI(adata.obs['louvain'], adata.obs[label_key]))\n",
    "# print(SC(adata.X, adata.obs['louvain'].values.reshape(-1, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    adtT,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    "    drop_last=True,\n",
    "    prefetch_factor=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled = np.random.permutation(np.arange(len(adtT)))[:10]\n",
    "valid_loader = DataLoader(\n",
    "    Subset(adtT, sampled),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:6\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model = scbasset.scBasset(n_cells=adtT.X.shape[1], hidden_size=z_dim, seq_len=seq_len, batch_ids=adtT.batche_ids).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load = 0\n",
    "\n",
    "# 100\n",
    "if not load:\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    scaler = GradScaler()\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer=optimizer,\n",
    "        mode=\"max\",\n",
    "        factor=0.95,\n",
    "        patience=2,\n",
    "        min_lr=1e-7\n",
    "    )\n",
    "\n",
    "    best_score = 0\n",
    "    wait = 0\n",
    "    patience = 15\n",
    "\n",
    "    best_sci = 0\n",
    "    best_embedding = None\n",
    "\n",
    "    max_epoch = max_epoch\n",
    "    for epoch in range(max_epoch):\n",
    "        pool = [np.nan for _ in range(10)]\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{max_epoch}\")\n",
    "        model.train()\n",
    "        for it, (seq, adt) in enumerate(pbar):\n",
    "            seq, adt = seq.to(device), adt.to(device)\n",
    "            # print(it,seq.shape,adt.shape);print()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            with autocast():\n",
    "                output = model(seq)[0]\n",
    "                # print(output.shape, adt.shape)\n",
    "                loss = criterion(output, adt)\n",
    "                # print(loss)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            pool[it % 10] = loss.item()\n",
    "\n",
    "            lr = optimizer.param_groups[-1][\"lr\"]\n",
    "            pbar.set_postfix_str(f\"loss/lr={np.nanmean(pool):.4f}/{lr:.3e}\")\n",
    "            # break\n",
    "        \n",
    "        \n",
    "        if epoch % 10 == 0: \n",
    "            pcc0, pcc1, sci, sci_res, embedding = test_model(model, valid_loader, device, epoch)\n",
    "            # pcc0, pcc1, sci, sci_res, embedding = test_model(model, train_loader, device, epoch)\n",
    "            \n",
    "            if sci > best_sci:\n",
    "                best_sci = sci\n",
    "                best_embedding = embedding\n",
    "\n",
    "        #     logger.info(\"Validation{} PCC0={:.4f} PCC1={:.4f} SC={:.4f}\".format((epoch + 1), pcc0, pcc1, sci))\n",
    "\n",
    "            # val_score = sci\n",
    "\n",
    "            # # scheduler.step(val_score)\n",
    "\n",
    "            # if val_score > best_score:\n",
    "            #     best_score = val_score\n",
    "            #     wait = 0\n",
    "            #     torch.save(model.state_dict(), \"{}/best_scb_ori_{}_{}_{}_{}_{}_{}.pt\".format(outdir, str(batch), str(seq_len), str(z_dim), str(lr * 1000), str(device), str(seed)))\n",
    "            #     logger.info(f\"Epoch {epoch+1}: best model saved\\n\")\n",
    "            # else:\n",
    "            #     wait += 1\n",
    "            #     if wait <= patience / 2:\n",
    "            #         embedding = model.get_embedding().detach().cpu().numpy().astype(np.float32)\n",
    "            #         sc.AnnData(embedding, obs=adtT.obs).write_h5ad(\"{}/best_scb_ori_emb_{}_{}_{}_{}_{}_{}.h5ad\".format(outdir, str(batch), str(seq_len), str(z_dim), str(lr * 1000), str(device), str(seed)))\n",
    "\n",
    "            #         logger.info(f\"Epoch {epoch+1}: early stopping patience {wait}/{patience}, embedding saved\\n\")\n",
    "            #     else:\n",
    "            #         logger.info(f\"Epoch {epoch+1}: early stopping patience {wait}/{patience}\\n\")\n",
    "            #     if wait >= patience:\n",
    "            #         logger.info(f\"Epoch {epoch+1}: early stopping\")\n",
    "            #         break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = best_embedding\n",
    "\n",
    "adata1 = sc.AnnData(\n",
    "    embedding,\n",
    "    obs=adtT.obs,\n",
    ")\n",
    "\n",
    "sc.pp.neighbors(adata1, use_rep='X')\n",
    "sc.tl.umap(adata1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.tl.louvain(adata1, random_state=seed)\n",
    "\n",
    "adata1.obs['louvain_res'] = find_res_label(adata1, len(np.unique(adata1.obs[label_key])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(adata1, color='louvain')\n",
    "sc.pl.umap(adata1, color=label_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ARI(adata1.obs['louvain'], adata1.obs[label_key]))\n",
    "print(NMI(adata1.obs['louvain'], adata1.obs[label_key]))\n",
    "print(cluster_acc(adata1.obs[label_key].to_numpy(), adata1.obs['louvain'].values.to_numpy()))\n",
    "print(FMI(adata1.obs['louvain'], adata1.obs[label_key]))\n",
    "# print(SC(adata1.X, adata1.obs['louvain'].values.reshape(-1, 1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rna2adt",
   "language": "python",
   "name": "rna2adt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
